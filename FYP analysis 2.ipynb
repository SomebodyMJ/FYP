{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb15e752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "723ec000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://www.computer.org/communities/technical-committees/tcmf', 'https://sigecom.org/', 'http://sigplan.org/', 'https://sigda.org/', 'https://sigcse.org/', 'https://www.kdd.org/', 'https://www.ieee-security.org/', 'https://sigmod.org/', 'http://sigir.org/', 'https://www.aclweb.org/portal/', 'https://sigact.org/', 'http://sigsac.org/', 'https://iclr.cc/', 'https://www.sigarch.org/', 'http://sigmetrics.org/', 'https://www.sighpc.org/', 'https://sigchi.org/', 'https://tc.computer.org/vgtc/', 'https://siggraph.org/', 'http://www.machinelearning.org/', 'https://siglog.org/', 'https://www.usenix.org/', 'https://aaai.org/', 'https://www.sigsoft.org/index.html', 'https://neurips.cc/', 'http://sigai.acm.org/index.html', 'https://sigmobile.org/', 'http://sigcomm.org/', 'https://sigbed.org/', 'https://www.iacr.org/', 'https://www.cv-foundation.org/', 'https://sigops.org/', 'http://www.sigbio.org/', 'https://www.ieee-ras.org/'}\n",
      "Processing URL: https://www.computer.org/communities/technical-committees/tcmf\n",
      "Processing URL: https://sigecom.org/\n",
      "Processing URL: http://sigplan.org/\n",
      "Processing URL: https://sigda.org/\n",
      "Processing URL: https://sigcse.org/\n",
      "Processing URL: https://www.kdd.org/\n",
      "Processing URL: https://www.ieee-security.org/\n",
      "Processing URL: https://sigmod.org/\n",
      "Processing URL: http://sigir.org/\n",
      "Processing URL: https://www.aclweb.org/portal/\n",
      "Processing URL: https://sigact.org/\n",
      "Processing URL: http://sigsac.org/\n",
      "Processing URL: https://iclr.cc/\n",
      "Processing URL: https://www.sigarch.org/\n",
      "Processing URL: http://sigmetrics.org/\n",
      "Processing URL: https://www.sighpc.org/\n",
      "Processing URL: https://sigchi.org/\n",
      "Processing URL: https://tc.computer.org/vgtc/\n",
      "Processing URL: https://siggraph.org/\n",
      "Processing URL: http://www.machinelearning.org/\n",
      "Processing URL: https://siglog.org/\n",
      "Processing URL: https://www.usenix.org/\n",
      "Processing URL: https://aaai.org/\n",
      "Processing URL: https://www.sigsoft.org/index.html\n",
      "Processing URL: https://neurips.cc/\n",
      "Processing URL: http://sigai.acm.org/index.html\n",
      "Processing URL: https://sigmobile.org/\n",
      "Processing URL: http://sigcomm.org/\n",
      "Processing URL: https://sigbed.org/\n",
      "Processing URL: https://www.iacr.org/\n",
      "Processing URL: https://www.cv-foundation.org/\n",
      "Processing URL: https://sigops.org/\n",
      "Processing URL: http://www.sigbio.org/\n",
      "Processing URL: https://www.ieee-ras.org/\n",
      "Number of links in link_call_all: 170\n",
      "First part done\n",
      "link_call_all\n",
      "link_call_all_1\n",
      "link_call_all_2\n",
      "Number of links in link_call_all: 169\n",
      "Number of links in link_call_all when 2024 is 24: 182\n",
      "Number of links in link_call_all when 2024 is not included: 1301\n",
      "Finish\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import (\n",
    "    StaleElementReferenceException, \n",
    "    TimeoutException, \n",
    "    NoSuchElementException\n",
    ")\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "# Part 1: Function to scrape links from a single URL with additional filtering\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def scrape_links(driver, url, retries=3):\n",
    "    def perform_scrape():\n",
    "        driver.get(url)\n",
    "        links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "        hrefs_1 = []\n",
    "\n",
    "        for link in links:\n",
    "            try:\n",
    "                href = link.get_attribute('href')\n",
    "                text_content = link.text  # Get the visible text of the link\n",
    "\n",
    "   \n",
    "                # Check in href, title, class, id, or text content\n",
    "                if href:\n",
    "                    if ('call' in (href or '').lower() or\n",
    "                        'call' in (text_content or '').lower() or\n",
    "                        '2024' in (href or '').lower() or\n",
    "                        '2024' in (text_content or '').lower()):\n",
    "                        hrefs_1.append(href)\n",
    "\n",
    "            except StaleElementReferenceException:\n",
    "                continue\n",
    "\n",
    "        return hrefs_1\n",
    "\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            hrefs_1 = perform_scrape()\n",
    "            return hrefs_1\n",
    "        except StaleElementReferenceException:\n",
    "            if attempt < retries:\n",
    "                print(f\"Retrying {url} due to stale element reference...\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(f\"Error scraping links from {url} : StaleElementReferenceException\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping links from {url} : {e}\")\n",
    "            return []\n",
    "        \n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "# Part 3: Scraping the links from the inital links\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    " \n",
    "    \n",
    "# Path to your Chrome WebDriver executable\n",
    "webdriver_path = r'C:\\Users\\Lee Ming Jia\\Desktop\\driver\\chromedriver-win64\\chromedriver.exe'\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode (without opening browser window)\n",
    "\n",
    "# Create a new Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=ChromeService(executable_path=webdriver_path), options=chrome_options)\n",
    "\n",
    "# Open the target webpage\n",
    "url = \"https://csrankings.org/#/fromyear/2024/toyear/2024/index?all&us\"\n",
    "driver.get(url)\n",
    "\n",
    "# Allow some time for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Locate the div with the specified class\n",
    "div_element = driver.find_element(By.CSS_SELECTOR, \"div.col-centered.col-xs-6.col-sm-push-6.col-sm-6.col-md-6.col-lg-6.text-center\")\n",
    "\n",
    "# Find all <p> tags with the specified class within the div\n",
    "p_elements = div_element.find_elements(By.CSS_SELECTOR, \"p.text-muted[style='font-variant:small-caps;']\")\n",
    "\n",
    "link_1 = set()\n",
    "\n",
    "# Extract and print the links from the <p> tags\n",
    "for p in p_elements:\n",
    "    links = p.find_elements(By.TAG_NAME, \"a\")\n",
    "    for link in links:\n",
    "        href = link.get_attribute(\"href\")\n",
    "        link_1.add(href)\n",
    "\n",
    "print(link_1)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "# Part 4: Scraping the rest of the links from the individual websites\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Initialize a set to store the collected links\n",
    "link_call = set()\n",
    "\n",
    "for url in link_1:\n",
    "    print(f\"Processing URL: {url}\")\n",
    "    scraped_links = scrape_links(driver, url)\n",
    "    link_call.update(scraped_links)  # Add scraped links to the link_call_all set\n",
    "\n",
    "num_links_1 = len(link_call)\n",
    "print(f\"Number of links in link_call: {num_links_1}\")\n",
    "\n",
    "print(\"First part done\")\n",
    "    \n",
    "# Print the collected links from the first scraping\n",
    "#print(\"Links from the first scraping (link_call):\")\n",
    "#print(link_call)\n",
    "\n",
    "# Step 2: Second round of scraping based on the links collected in link_call\n",
    "#link_call_all = set(link_call)  # Initialize the new set with the links from the first scraping\n",
    "\n",
    "link_call_all = set()\n",
    "link_call_all_1 = set()\n",
    "link_call_all_2 = set()\n",
    "\n",
    "print('link_call_all')\n",
    "for url in link_call:\n",
    "    #print(f\"Processing scraped URL: {url}\")\n",
    "    \n",
    "    # Navigate to the URL using Selenium\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Optionally, check if the URL itself contains '2024' (you can adjust this condition as needed)\n",
    "\n",
    "    scraped_links = scrape_links(driver, url)\n",
    "        \n",
    "    # Add only the links that contain '2024'\n",
    "    for link in scraped_links:\n",
    "        if '2024' in link and 'call' in link.lower():  # Check both '2024' and case-insensitive 'call'\n",
    "            link_call_all.add(link)\n",
    "\n",
    "print('link_call_all_1')\n",
    "for url in link_call:\n",
    "    #print(f\"Processing scraped URL: {url}\")\n",
    "    \n",
    "    # Navigate to the URL using Selenium\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Optionally, check if the URL itself contains '2024' (you can adjust this condition as needed)\n",
    "\n",
    "    scraped_links = scrape_links(driver, url)\n",
    "        \n",
    "    # Add only the links that contain '2024'\n",
    "    for link in scraped_links:\n",
    "        if '24' in link and 'call' in link.lower():  # Check both '2024' and case-insensitive 'call'\n",
    "            link_call_all_1.add(link)        \n",
    "\n",
    "print('link_call_all_2')\n",
    "for url in link_call:\n",
    "    #print(f\"Processing scraped URL: {url}\")\n",
    "    \n",
    "    # Navigate to the URL using Selenium\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Optionally, check if the URL itself contains '2024' (you can adjust this condition as needed)\n",
    "\n",
    "    scraped_links = scrape_links(driver, url)\n",
    "        \n",
    "    # Add only the links that contain '2024'\n",
    "    for link in scraped_links:\n",
    "        if 'call' in link.lower():  # Check both '2024' and case-insensitive 'call'\n",
    "            link_call_all_2.add(link)\n",
    "\n",
    "\n",
    "            \n",
    "# Final output\n",
    "#print(\"All collected links from the second round restricted to 2024 (link_call_all):\")\n",
    "#for link in link_call_all:\n",
    "#    print(link)\n",
    "\n",
    "num_links_2 = len(link_call_all)\n",
    "print(f\"Number of links in link_call_all: {num_links_2}\")\n",
    "num_links_3 = len(link_call_all_1)\n",
    "print(f\"Number of links in link_call_all when 2024 is 24: {num_links_3}\")\n",
    "num_links_4 = len(link_call_all_2)\n",
    "print(f\"Number of links in link_call_all when 2024 is not included: {num_links_4}\")\n",
    "# Close the WebDriver after scraping is done\n",
    "driver.quit()\n",
    "\n",
    "print('Finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "483daf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_call_all_3\n",
      "Number of links in link_call_all when call is not included but 2024 is: 1090\n",
      "Finish\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Path to your Chrome WebDriver executable\n",
    "webdriver_path = r'C:\\Users\\Lee Ming Jia\\Desktop\\driver\\chromedriver-win64\\chromedriver.exe'\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode (without opening browser window)\n",
    "\n",
    "# Create a new Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=ChromeService(executable_path=webdriver_path), options=chrome_options)\n",
    "\n",
    "link_call_all_3 = set()\n",
    "\n",
    "print('link_call_all_3')\n",
    "for url in link_call:\n",
    "    #print(f\"Processing scraped URL: {url}\")\n",
    "    \n",
    "    # Navigate to the URL using Selenium\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Optionally, check if the URL itself contains '2024' (you can adjust this condition as needed)\n",
    "\n",
    "    scraped_links = scrape_links(driver, url)\n",
    "        \n",
    "    # Add only the links that contain '2024'\n",
    "    for link in scraped_links:\n",
    "        if '2024' in link:  # Check both '2024' and case-insensitive 'call'\n",
    "            link_call_all_3.add(link)\n",
    "\n",
    "            \n",
    "num_links_5 = len(link_call_all_3)\n",
    "print(f\"Number of links in link_call_all when call is not included but 2024 is: {num_links_5}\")\n",
    "# Close the WebDriver after scraping is done\n",
    "driver.quit()\n",
    "\n",
    "print('Finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fdc68f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
